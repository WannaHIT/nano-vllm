import torch
from flash_attn import flash_attn_func

# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„CUDAè®¾å¤‡
if not torch.cuda.is_available():
    print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDA GPUã€‚FlashAttentionæ— æ³•è¿è¡Œã€‚")
else:
    print(f"æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
    
    # åˆ›å»ºä¸€äº›éœ€è¦åœ¨GPUä¸Šçš„è™šæ‹Ÿæ•°æ®
    # å°ºå¯¸: (æ‰¹å¤§å°, å¤´æ•°é‡, åºåˆ—é•¿åº¦, å¤´ç»´åº¦)
    q = torch.randn(2, 4, 128, 64, device='cuda', dtype=torch.float16)
    k = torch.randn(2, 4, 128, 64, device='cuda', dtype=torch.float16)
    v = torch.randn(2, 4, 128, 64, device='cuda', dtype=torch.float16)

    print("æµ‹è¯•æ•°æ®å·²åˆ›å»ºï¼Œå‡†å¤‡è°ƒç”¨FlashAttentionæ ¸å¿ƒå‡½æ•°...")

    # è°ƒç”¨flash_attnçš„æ ¸å¿ƒå‡½æ•°
    try:
        output = flash_attn_func(q, k, v)
        print("\nğŸ‰ æ­å–œï¼FlashAttention æ ¸å¿ƒå‡½æ•°è¿è¡ŒæˆåŠŸï¼")
        print(f"è¾“å‡ºå¼ é‡çš„å½¢çŠ¶ (Output shape): {output.shape}")
        print(f"è¾“å‡ºå¼ é‡çš„æ•°æ®ç±»å‹ (Output dtype): {output.dtype}")
    except Exception as e:
        print("\nâŒ å‡ºé”™äº†ï¼è¿è¡ŒFlashAttentionæ ¸å¿ƒå‡½æ•°æ—¶é‡åˆ°é—®é¢˜ï¼š")
        print(e)
